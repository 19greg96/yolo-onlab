[net]
# Testing
# batch=1
# subdivisions=1
# Training
batch=64
subdivisions=8
width=416
height=416
channels=3
momentum=0.9
decay=0.0005

# ---------tests to run:
# batch = 32, 64, 128
# Adam
# Dropout
# ---------this test configuration:
# batch = 64
# Adam = 1
# Dropout = 0.1

adam=1
B1=0.9
B2=0.999
eps=0.000001

# maximum angle in radians that images will be rotated by.
# 45Â° = 0.785398
angle=0.2
# saturation, exposure and hue values - ranges for random changes of colours of images during training (params for data augumentation), in terms of HSV: https://en.wikipedia.org/wiki/HSL_and_HSV
# The larger the value, the more invariance would neural network to change of lighting and color of the objects.
saturation = 1.5
exposure = 1.5
hue=.1
# jitter can be [0-1] and used to crop images during training for data augumentation. The larger the value of jitter, the more invariance would neural network to change of size and aspect ratio of the objects.
jitter=0.2
# flip=1 means probability of flipping an image is 50%
flip=1
# amount of noise to be added to images. For each image a new value is generated: image_noise = rand_normal() * noise
noise=0.2

learning_rate=0.005
burn_in=1000
max_batches = 60024
policy=steps
# steps is a checkpoints (number of itarations) at which scales will be applied
steps=40016,50020
# scales is a coefficients at which learning_rate will be multipled at this checkpoints.
scales=.1,.1

[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[dropout]
probability=.1

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=1

[dropout]
probability=.1

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

###########

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear



[yolo]
mask = 3,4,5
# anchors are frequent initial <width,height> of objects in terms of output network resolution.
# If you train with height=416,width=416,random=0, then max values of anchors will be 13,13.
# But if you train with random=1, then max input resolution can be 608x608, and max values of anchors can be 19,19.
anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
classes=1
num=6
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 8

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear

[yolo]
mask = 0,1,2
anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
classes=1
num=6
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
